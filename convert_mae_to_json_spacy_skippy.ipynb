{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:49.871518Z",
     "start_time": "2021-04-01T17:44:49.868732Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import date\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import spacy\n",
    "from copy import deepcopy\n",
    "from mae_utils import parse_mae\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_tokenization_init import make_customize_tokenizer_pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "Doc.set_extension(\"id\", default=None, force=True)\n",
    "Doc.set_extension(\"note_id\", default=None, force=True)\n",
    "Doc.set_extension(\"note_type\", default=None, force=True)\n",
    "Doc.set_extension(\"fold\", default=None, force=True)\n",
    "Doc.set_extension(\"dataset\", default=None, force=True)\n",
    "Doc.set_extension(\"annotator\", default=None, force=True)\n",
    "Doc.set_extension(\"entities\", default=None, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "def modification_date(filename):\n",
    "    t = os.path.getmtime(filename)\n",
    "    return datetime.datetime.fromtimestamp(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:49.875385Z",
     "start_time": "2021-04-01T17:44:49.873569Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_filename(fn, pattern_filename = re.compile(\"(?P<note_id>\\d+)(?:_(?P<annotator>[A-Za-z]+))\\.xml\")\n",
    "                  ):\n",
    "    match = pattern_filename.search(fn)\n",
    "    if match:\n",
    "        fn_dict = match.groupdict()\n",
    "        fn_dict[\"note_id\"] = int(fn_dict[\"note_id\"])\n",
    "    else:\n",
    "        fn_dict = {\"note_id\":None, \"annotator\":None}\n",
    "    return fn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# move the NER component to the end of the pipeline: remove and then reload from the same source in the new position\n",
    "nlp.remove_pipe(\"ner\")\n",
    "nlp.add_pipe(\"ner\", source=spacy.load(\"en_core_web_sm\"))\n",
    "\n",
    "# add entity ruler\n",
    "# nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp = make_customize_tokenizer_pathology()(nlp)\n",
    "# nlp.add_pipe(\"morphologizer\")\n",
    "\n",
    "# nlp.initialize()\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add matcher patterns from manually edited json file (note still on 210920_null directory)\n",
    "with open(\"data/domain_patterns_dict_manedit.json\") as fh:\n",
    "    domain_patterns_imp = json.load(fh)\n",
    "\n",
    "\n",
    "####################    added lines  ############################\n",
    "matcher_lvl1 = Matcher(nlp.vocab)\n",
    "for domain in domain_patterns_imp:\n",
    "    matcher_lvl1.add(domain, domain_patterns_imp[domain])\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"Social_isolation\", \n",
    "            [[{\n",
    "                \"LOWER\": {\"IN\":[\"live\", \"living\", \"lives\",\n",
    "                                \"stay\",\"stays\", \"staying\", \n",
    "                                \"reside\", \"resides\", \"residing\",\n",
    "                                \"housed\", \"house\",]},\n",
    "            },\n",
    "            {\"LOWER\": {\"IN\":[\"in\", \"at\"]}, \"OP\":\"?\"},\n",
    "            {\"OP\":\"*\"},\n",
    "            {\n",
    "                \"LOWER\": {\n",
    "                    \"IN\": [\n",
    "#                         \"in\",\n",
    "                        \"together\",\n",
    "                        \"with\",\n",
    "                        \"w/\",\n",
    "                        \"w\",\n",
    "                        \"by\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            {\"LOWER\": {\"IN\":[\"his\", \"her\", \"their\", \"herself\", \"himself\"]}, \"OP\":\"?\"},\n",
    "            ],\n",
    "            [{\"LOWER\": {\"IN\":[\"moves\", \"moved\", \"move\"]}},\n",
    "             {\"LOWER\": {\"IN\":[\"into\", \"in\", \"with\"]}}\n",
    "            ]\n",
    "            ]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates1 = pd.read_csv(\"data/annotation-dates-batch1.csv\")\n",
    "dates1.datetime = dates1.datetime.map(datetime.datetime.fromisoformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates1.dtypes\n",
    "# lambda x: {\"Raf\": \"Rafael\",  \"Mat\": \"Matt\"}.get(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:49.907682Z",
     "start_time": "2021-04-01T17:44:49.877505Z"
    }
   },
   "outputs": [],
   "source": [
    "indir  = \"data/skippy-2020-02-12-per-type\"   # I edited this for my local system\n",
    "\n",
    "tag_list = []\n",
    "data = []\n",
    "for dd in os.scandir(indir):\n",
    "    print(dd.name)\n",
    "    for ff in os.scandir(dd):\n",
    "        if not ff.name.endswith(\"xml\") or not \"_\" in ff.name:\n",
    "            continue        \n",
    "        text, tag_list_ = parse_mae(ff.path, skip_errors=False)\n",
    "        entry = {\"text\": text,\n",
    "                 \"entities\": tag_list_,\n",
    "                 \"note_type\": dd.name\n",
    "                }\n",
    "        entry.update(parse_filename(ff.name))\n",
    "        entry[\"filename\"]  = ff.name\n",
    "        entry[\"annotator\"] = {\"Raf\": \"Rafael\",  \n",
    "                              \"Rafal\": \"Rafael\",\n",
    "                              \"Rafae\": \"Rafael\",\n",
    "                              \"Mtt\": \"Matt\",\n",
    "                              \"Mat\": \"Matt\"}.get(entry[\"annotator\"], entry[\"annotator\"])\n",
    "        entry[\"annotator\"] = entry[\"annotator\"].title() if entry[\"annotator\"] is not None else None\n",
    "        entry[\"datetime\"] = modification_date(ff)\n",
    "        if entry[\"annotator\"] is None:\n",
    "            continue\n",
    "        data.append(entry)\n",
    "        for tag in tag_list_:\n",
    "            tag[\"filename\"] = ff.name\n",
    "            tag[\"note_type\"] = dd.name\n",
    "        tag_list.extend(tag_list_)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_counts = []\n",
    "for doc in data:\n",
    "    if doc[\"annotator\"].title()==\"Sam\":\n",
    "        continue\n",
    "    _c_ = {kk:doc[kk] for kk in [\"note_id\", \"annotator\"]}\n",
    "    _c_[\"n_entities\"] = len(doc[\"entities\"])\n",
    "    _c_[\"len\"] = len(doc[\"text\"])\n",
    "    _c_[\"note_type\"] = (doc[\"note_type\"])\n",
    "    note_counts.append(_c_)\n",
    "\n",
    "note_counts = (pd.DataFrame(note_counts))\n",
    "note_counts[\"annotator\"] = note_counts[\"annotator\"].str.title()\n",
    "\n",
    "note_counts_wide =  note_counts.set_index([\"note_id\", \"annotator\"])[\"n_entities\"].unstack()\n",
    "note_counts_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_counts_wide.loc[(~note_counts_wide.isnull()).sum(1)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_counts_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_stats = pd.DataFrame({\"n_entities\": note_counts_wide.mean(1),\n",
    " \"note_len\": note_counts.groupby(\"note_id\")[\"len\"].mean().astype(int),\n",
    " \"note_type\": note_counts.groupby(\"note_id\")[\"note_type\"].first()\n",
    "             }\n",
    "            )\n",
    "note_stats.to_csv(\"results/note_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_annotations = (~note_counts_wide.loc[((~note_counts_wide.isnull()).any(1) &\n",
    "                       (note_counts_wide.sum(1)>0))].isnull()).sum(1).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = num_of_annotations[::-1].plot(kind=\"barh\")\n",
    "ax.set_xlim([0, max(num_of_annotations)+70])\n",
    "\n",
    "for index, row in num_of_annotations[::-1].items():\n",
    "    ax.text(row+30, 4-index, row, color='black', ha=\"center\", fontsize=14)\n",
    "\n",
    "ax.set_xlabel(\"number of documents\")\n",
    "ax.set_ylabel(\"number of annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "626/1576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (~note_counts_wide.isnull()).any(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (note_counts_wide>0).any(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(\n",
    "#     note_annotator,\n",
    "#     note_annotator[\"note_id\"].value_counts()[note_annotator[\"note_id\"].value_counts()<2],\n",
    "#     right_index=True,\n",
    "#     left_on = \"note_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotchecking and correction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for entry in data:\n",
    "#     for ent in entry[\"entities\"]:\n",
    "#         if ent[\"type\"] ==\"Mixed anxiety and depressive disorder\":\n",
    "#             raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    if entry[\"filename\"] == \"668198482_Sara.xml\":\n",
    "        break\n",
    "#     _Matt.xml\"\n",
    "doc = nlp(entry[\"text\"])\n",
    "print(entry[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 0\n",
    "for tok in doc:\n",
    "    if tok.text == \"personal\":\n",
    "        nn+=1\n",
    "        if nn > 0:\n",
    "            break\n",
    "nn, tok, tok.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = doc.char_span(tok.idx, tok.idx+1, label=\"a\", alignment_mode=\"expand\")\n",
    "longer_ent = doc[ent.start: ent.end+1]\n",
    "print(longer_ent)\n",
    "longer_ent.start_char, longer_ent.end_char\n",
    "print(f'spans=\"{longer_ent.start_char+1}~{longer_ent.end_char+1}\" text=\"{longer_ent}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    for ent in entry[\"entities\"]:\n",
    "        if (ent[\"label\"] == \"Housing\") & (ent[\"type\"]==\"Housed\"):\n",
    "            if ent[\"text\"] == 'lives alone':\n",
    "                ent[\"label\"] = \"Social_isolation\"\n",
    "                ent[\"type\"] = \"Lives alone\"\n",
    "            else:\n",
    "                ent[\"type\"] = \"Stably housed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sdoh-cocoa-skippy-raw.jsonl\", \"w\") as fh:\n",
    "    for line in data:\n",
    "        line = deepcopy(line)\n",
    "        line[\"datetime\"] = line[\"datetime\"].isoformat()\n",
    "        fh.write(json.dumps(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dumps(line)\n",
    "# line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = f\"{indir}/Telephone_Encounter/769128562_Sara.xml\"\n",
    "# parse_mae(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_labels = pd.DataFrame(sum([row[\"entities\"] for row in data], []))[[\"label\", \"type\"]].drop_duplicates()\n",
    "uniq_labels.groupby(\"type\")[\"label\"].agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1)\n",
    "# ax.scatter(dates1[\"datetime\"], dates1[\"annotator\"])\n",
    "\n",
    "# ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:49.907682Z",
     "start_time": "2021-04-01T17:44:49.877505Z"
    }
   },
   "outputs": [],
   "source": [
    "# indir  = \"data/skippy-2020-02-12\"   # I edited this for my local system\n",
    "\n",
    "# tag_list = []\n",
    "# data = []\n",
    "\n",
    "# for ff in os.scandir(indir):\n",
    "#     if not ff.name.endswith(\"xml\") or not \"_\" in ff.name:\n",
    "#         continue        \n",
    "#     text, tag_list_ = parse_mae(ff.path)\n",
    "#     entry = {\"text\": text,\n",
    "#              \"entities\": tag_list_,\n",
    "#              \"note_type\": dd.name\n",
    "#             }\n",
    "#     entry.update(parse_filename(ff.name))\n",
    "#     entry[\"filename\"]  = ff.name\n",
    "#     entry[\"annotator\"] = {\"Raf\": \"Rafael\",  \n",
    "#                           \"Rafal\": \"Rafael\",\n",
    "#                           \"Rafae\": \"Rafael\",\n",
    "#                           \"Mtt\": \"Matt\",\n",
    "#                           \"Mat\": \"Matt\"}.get(entry[\"annotator\"], entry[\"annotator\"])\n",
    "#     entry[\"annotator\"] = entry[\"annotator\"].title() if entry[\"annotator\"] is not None else None\n",
    "#     entry[\"datetime\"] = modification_date(ff)\n",
    "#     if entry[\"annotator\"] is None:\n",
    "#         continue\n",
    "#     data.append(entry)\n",
    "#     for tag in tag_list_:\n",
    "#         tag[\"filename\"] = ff.name\n",
    "# #         tag[\"note_type\"] = dd.name\n",
    "#     tag_list.extend(tag_list_)\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([entry[\"datetime\"] for entry in data]).value_counts().plot()\n",
    "# plt.ylim([0, 18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,)\n",
    "\n",
    "for annotator, vv in (\n",
    "    pd.DataFrame([{\"datetime\":entry[\"datetime\"], \"annotator\": entry[\"annotator\"]} for entry in data])\n",
    "    .value_counts([\"annotator\", \"datetime\"])\n",
    "    .groupby(\"annotator\")\n",
    "\n",
    "):\n",
    "    ax.plot(vv[annotator], marker=\".\", ls=\"\", label=annotator)\n",
    "    \n",
    "ax.set_ylim([0, 6])\n",
    "ax.set_xlim([datetime.datetime.fromisoformat(\"2021-02-01\"),\n",
    "          datetime.datetime.fromisoformat(\"2021-07-15\")])\n",
    "\n",
    "xticks = sum([[datetime.datetime.fromisoformat(f\"2021-{mm:02d}-01\"),\n",
    " datetime.datetime.fromisoformat(f\"2021-{mm:02d}-15\")] for mm in range(4,8)], [])\n",
    "\n",
    "xticks = [datetime.datetime.fromisoformat(f\"2021-{mm:02d}-01\") for mm in range(1,8)]\n",
    "\n",
    "# plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator())\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([{\"filename\": entry[\"filename\"], \"datetime\":entry[\"datetime\"], \"annotator\": entry[\"annotator\"]} \n",
    "#               for entry in data]).to_csv(\"data/annotation-dates-batch1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sdoh-cocoa-skippy-all-2022-01-07.jsonl\", \"w\") as fh:\n",
    "    for entry in data:\n",
    "        entry[\"datetime\"] = entry[\"datetime\"].strftime(\"%Y-%m-%d\")\n",
    "        fh.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(entry):\n",
    "    output = {kk:vv for kk, vv in entry.items() if kk not in {\"text\", \"entities\"}}\n",
    "    output.update({\"n_entities\": len(entry[\"entities\"])})\n",
    "    return output\n",
    "\n",
    "metadata = pd.DataFrame([get_meta(entry)\n",
    "                         for entry in data]).set_index(\"note_id\")\n",
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby(level=\"note_id\").count()[\"filename\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata.groupby(\"note_type\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby(\"annotator\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.index.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby(level=0)[\"n_entities\"].max().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby(\"note_type\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby([\"note_type\", \"annotator\"]).sum()[\"n_entities\"].unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.index.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.reset_index().groupby([\"note_type\", \"note_id\"])[\"n_entities\"].mean()[metadata.reset_index().groupby([\"note_type\", \"note_id\"])[\"n_entities\"].mean()>0\n",
    "                                                                             ].groupby(\"note_type\").sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[metadata.n_entities>0].index.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids = pd.concat({\"counts\": metadata.index.value_counts(),\n",
    "                      \"n_entities_max\": metadata.groupby(level=0)[\"n_entities\"].max()},1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids[\"n_entities_bucket\"] = pd.cut(note_ids[\"n_entities_max\"], [-1,0,1,5,10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids[\"buckets\"] = note_ids.apply(lambda row: (row[\"counts\"], row[\"n_entities_bucket\"].left, row[\"n_entities_bucket\"].right), 1)\n",
    "note_ids[\"buckets\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids.loc[note_ids[\"buckets\"].map(lambda row: row[0]==5), \"buckets\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids[\"buckets_large\"] = note_ids[\"buckets\"].map(lambda row: {(5, 0, 1):(4, 0, 1),\n",
    "                                     (5, 10, 100): (None, 10, 100),\n",
    "                                     (4, 10, 100): (None, 10, 100),\n",
    "                                     (3, 10, 100): (None, 10, 100),\n",
    "#                                      (2, 10, 100): (None, 10, 100),\n",
    "                                    }.get(row, row)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note_ids.loc[note_ids[\"buckets\"].map(lambda row: row[0]==5), \"buckets\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(6)\n",
    "note_id_set = pd.Series({ii:note_ids.index[vv] for ii, (tt,vv)\n",
    "                         in enumerate(skf.split(note_ids.index, note_ids[\"buckets_large\"].astype(str)))}).explode()\n",
    "note_id_set.index.name =  \"fold\"\n",
    "note_id_set.name =  \"note_id\"\n",
    "note_id_set = note_id_set.reset_index()\n",
    "note_id_set[\"set\"] = note_id_set[\"fold\"].map(lambda x: {4:\"val\", 5:\"test\"}.get(x,\"train\"))\n",
    "note_id_set = note_id_set.set_index(\"note_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_id_set[\"set\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"set\" not in metadata.columns:\n",
    "    metadata = pd.merge(metadata, note_id_set, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.index.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[metadata.n_entities>0].index.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv(\"data/skippy-2020-02-12-data-splits-2022-01-11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import span_utils\n",
    "import overlap\n",
    "\n",
    "import importlib\n",
    "span_utils = importlib.reload(span_utils)\n",
    "overlap = importlib.reload(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_whitespaces(spans):\n",
    "    for sp in spans:\n",
    "        if (doc[sp.end].is_space):\n",
    "            for ii in range(sp.end, sp.start-1, -1):\n",
    "                if not (doc[ii].is_space):\n",
    "                    sp.end = ii\n",
    "                    break\n",
    "        if (doc[sp.start].is_space):\n",
    "            for ii in range( sp.start, sp.end):\n",
    "                if not (doc[ii].is_space):\n",
    "                    sp.start = ii\n",
    "                    break\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_whitespaces_dict(spans, text,\n",
    "                         chars = {\" \", \"\\n\", \"\\t\", \",\", \":\", \".\", \";\", \"-\"}):\n",
    "    for sp in spans:\n",
    "        if (text[sp[\"end\"]] in chars):\n",
    "            for ii in range(sp[\"end\"], sp[\"start\"]-1, -1):\n",
    "                if not (text[ii] in chars):\n",
    "                    sp[\"end\"] = ii+1\n",
    "                    break\n",
    "        if (text[sp[\"start\"]] in chars):\n",
    "            for ii in range( sp[\"start\"], sp[\"end\"]):\n",
    "                if not (text[ii] in chars):\n",
    "                    sp[\"start\"] = ii\n",
    "                    break\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def split_overlapping_spans(snippet, labels = []):\n",
    "    matches = matcher_lvl1(snippet, with_alignments = True)\n",
    "    spans = defaultdict(lambda : [])\n",
    "    for hash_, start, end, algn in matches:\n",
    "        label = matcher_lvl1.vocab.strings[hash_]\n",
    "        span = snippet[start: end]\n",
    "        span.label_ = label\n",
    "        spans[label].append((span, algn[-1] - algn[0]))\n",
    "        \n",
    "    new_spans = []\n",
    "    for kk, vv in spans.items():\n",
    "        if len(labels) and kk not in labels:\n",
    "            continue\n",
    "        vv = spacy.util.filter_spans([x[0] for x in vv])\n",
    "#         vv = sorted(enumerate(vv), key = lambda x: x[1][1])[-1][1][0]\n",
    "        new_spans.extend(vv)\n",
    "    return new_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snippet = nlp(\"Lives at home with\")\n",
    "# snippet = nlp(\"Sister and half brother with depression\")\n",
    "# spans_disamb = split_overlapping_spans(snippet, labels =  [\"Social_isolation\", \"Depression\"])\n",
    "# # spans_disamb = sorted(spans_disamb, key = lambda x : ( x.end))\n",
    "# spans_disamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Span.set_extension(\"type\", default=None)\n",
    "except Exception as ee:\n",
    "    print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def recover_lives_with(sp, margin = 20, matcher=matcher):\n",
    "    doc = sp.doc\n",
    "    snippet = doc[sp.start-margin: sp.end+margin]\n",
    "    matches = matcher(snippet, with_alignments=True)\n",
    "#     return matches\n",
    "    if len(matches)>0:\n",
    "        index = max(enumerate([alignment[-1] for _, start, end, alignment in matches]), \n",
    "                    key=lambda x: x[1])[0]\n",
    "        _, start, end, alignment = matches[index]\n",
    "#         logging.debug(index)\n",
    "#         max_len_ = 0\n",
    "#         longest_span = None\n",
    "#         for _, start, end in matches:\n",
    "# #         match = matches[0]\n",
    "        longest_span = doc[sp.start-margin + start: sp.start-margin + end]\n",
    "#         if len(temp_span)> max_len_:\n",
    "#             longest_span = temp_span\n",
    "#         max_len_ = len(temp_span)\n",
    "        longest_span.label = sp.label\n",
    "        for attr in dir(sp._):\n",
    "            if attr not in {'get', 'has', 'set'}:\n",
    "                longest_span._.set(attr, getattr(sp._, attr))\n",
    "\n",
    "        return longest_span\n",
    "    else:\n",
    "        logging.warning(f\"no matches in: '{snippet}'\")\n",
    "#         raise Exception\n",
    "        return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snippet = nlp('''for laboratory services at UCSF Center for Reproductive Health.    \n",
    "# Lives in San Francisco with mother, brother. Lives in Teotihuacan on the weekends with her boyfriend.   \n",
    "# Exercise: 60-90 min on 3 days/''')[0:]\n",
    "# matcher(snippet, with_alignments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover_lives_with(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_dict_lives_with(text, start, end, margin = 20, matcher=matcher):\n",
    "    doc = sp.doc\n",
    "    snippet = text[sp.start-margin: sp.end+margin]\n",
    "    matches = matcher(snippet)\n",
    "    if len(matches)>0:\n",
    "        index = max(enumerate([alignment[-1] for _, start, end, alignment in matches]), key=lambda x: x[1])[0]\n",
    "        _, start, end, alignment = matches[index]\n",
    "        print(index)\n",
    "#         max_len_ = 0\n",
    "#         longest_span = None\n",
    "#         for _, start, end in matches:\n",
    "# #         match = matches[0]\n",
    "        temp_span = doc[sp.start-margin + start: sp.start-margin + end]\n",
    "        if len(temp_span)> max_len_:\n",
    "            longest_span = temp_span\n",
    "        max_len_ = len(temp_span)\n",
    "        longest_span.label = sp.label\n",
    "        for attr in dir(sp._):\n",
    "            if attr not in {'get', 'has', 'set'}:\n",
    "                longest_span._.set(attr, getattr(sp._, attr))\n",
    "\n",
    "        return longest_span\n",
    "    else:\n",
    "        logging.warning(f\"no matches in: '{snippet}'\")\n",
    "#         raise Exception\n",
    "        return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cython\n",
    "# from libcpp.vector cimport vector\n",
    "# def overlap(int x1, int x2, int y1, int y2):\n",
    "# #     if x1 < y2 or y1<x2:\n",
    "#     length = max(0, max(x2, y2) - min(x1,y1))\n",
    "#     if (length)>0:\n",
    "#         return max(0, (min(x2, y2) - max(x1,y1)) / length)\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "def get_priority(x, y, priority_dict):\n",
    "    return x if priority_dict[x] <= priority_dict[y] else y\n",
    "    \n",
    "def clean_overlaps_disjoin(spans, priority_dict = {}):\n",
    "    sp_prev = None\n",
    "#     cdef vector[int] remove_list\n",
    "    remove_list  = []\n",
    "    for ii, sp in enumerate(spans):\n",
    "\n",
    "        if sp_prev is None:\n",
    "            sp_prev = sp\n",
    "            continue\n",
    "\n",
    "        ov = overlap.overlap(sp[\"start\"], sp[\"end\"], sp_prev[\"start\"], sp_prev[\"end\"])\n",
    "#         print(ov, sp[\"start\"], sp[\"end\"], sp_prev[\"start\"], sp_prev[\"end\"], sp[\"text\"])\n",
    "        if ov > 0.0:\n",
    "            if abs(sp[\"end\"] - sp_prev[\"end\"]) > abs(sp[\"start\"] - sp_prev[\"start\"]): \n",
    "                if sp[\"end\"] > sp_prev[\"end\"]:\n",
    "                    sp[\"start\"] = sp_prev[\"end\"]\n",
    "                else:\n",
    "                    sp_prev[\"start\"] = sp[\"end\"]\n",
    "#                 remove_list.push_back(ii-1)\n",
    "#                 remove_list.append(ii-1)\n",
    "            elif abs(sp[\"end\"] - sp_prev[\"end\"]) > abs(sp[\"start\"] - sp_prev[\"start\"]):\n",
    "                if sp[\"start\"] < sp_prev[\"start\"]:\n",
    "                    sp[\"end\"] = sp_prev[\"start\"]\n",
    "                else:\n",
    "                    sp_prev[\"end\"] = sp[\"start\"]\n",
    "#                 remove_list.push_back(ii)\n",
    "#                 remove_list.append(ii)\n",
    "            else: \n",
    "                logging.debug(f\"ties:{sp}\\n{sp_prev}\")\n",
    "                if len(priority_dict)>0:\n",
    "                    if priority_dict.get(sp[\"label\"], 1e20) <= priority_dict.get(sp_prev[\"label\"], 1e20):\n",
    "#                         sp_prev[\"label\"] = None\n",
    "#                         remove_list.push_back(ii-1)\n",
    "                        remove_list.append(ii-1)\n",
    "                    else:\n",
    "#                         sp[\"label\"] = None\n",
    "                        remove_list.append(ii)\n",
    "        sp_prev = sp\n",
    "#     print(\"remove_list\", remove_list)\n",
    "    for ii in reversed(remove_list):\n",
    "        spans.pop(ii)\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_overlaps_disjoin_spacy(spans, priority_dict = {},\n",
    "                                 label_fn = lambda x: x.split(': ')[0]):\n",
    "    doc = spans[0].doc\n",
    "    sp_prev = None\n",
    "#     cdef vector[int] remove_list\n",
    "    remove_list  = []\n",
    "    new_spans = []\n",
    "    prev_ranges = []\n",
    "    \n",
    "    for ii, sp in enumerate(spans):\n",
    "        \n",
    "#         if (\"daughter\" in sp.text) and (\"daughter\" != sp.text.lower()):\n",
    "#             raise Exception\n",
    "            \n",
    "        if sp_prev is None:\n",
    "            sp_prev = sp\n",
    "            new_spans.append(sp)\n",
    "            prev_ranges.append((sp.start, sp.end))\n",
    "            continue\n",
    "\n",
    "        ov = overlap.overlap(sp.start, sp.end, sp_prev.start, sp_prev.end)\n",
    "        #logging.debug(f\"{sp_prev.start} -- {sp_prev.end}: {sp.start} -- {sp.end}, overlap = {ov}\")\n",
    "        #logging.debug(f\"this one:{abs(sp.end - sp.start)}\\nprevios:{abs(sp_prev.end - sp_prev.start)}\")\n",
    "#         print(ov, sp.start, sp.end, sp_prev.start, sp_prev.end, sp[\"text\"])\n",
    "        if ov > 0.0:\n",
    "            if sp_prev.label == sp.label:\n",
    "                this_len = sp.end - sp.start\n",
    "                prev_len = sp_prev.end - sp_prev.start\n",
    "                if this_len > prev_len:\n",
    "                    logging.debug(f\"keeping the longer: {sp} ({sp.label_})\")\n",
    "                    remove_list.append(ii-1)\n",
    "                else:\n",
    "                    logging.debug(f\"keeping the longer: {sp_prev} ({sp_prev.label_})\")\n",
    "                    remove_list.append(ii)\n",
    "                if sp_prev is not None:\n",
    "                    new_spans[-1] = sp_prev\n",
    "                new_spans.append(sp)\n",
    "                sp_prev = sp\n",
    "                prev_ranges.append((sp.start, sp.end))\n",
    "                continue\n",
    "            # tail overhang is longer then head overhang\n",
    "            relevant_starts = [s for s, e in prev_ranges if e > sp.start]\n",
    "            logging.debug(f\"relevant_starts: {relevant_starts}\")\n",
    "            tail_overhang = abs(sp.end - sp_prev.end)\n",
    "            head_overhang = abs(sp.start - min(relevant_starts))\n",
    "            flag_tail = (sp.label_ == \"Social_isolation: Lives with\") or \\\n",
    "                (sp_prev.label_ == \"Social_isolation: Lives with\") and  sp.end != sp_prev.end\n",
    "            \n",
    "            if tail_overhang > head_overhang or flag_tail:\n",
    "                logging.debug(f\"tail overhang ({tail_overhang}) is longer then head overhang({head_overhang})\")\n",
    "                logging.debug(f\"fixing tails\")\n",
    "                if sp.end > sp_prev.end:\n",
    "#                     sp.start = sp_prev.end\n",
    "                    label = sp.label_\n",
    "                    sp = doc[sp_prev.end:sp.end]\n",
    "                    sp.label_ =  label\n",
    "                    logging.debug(f\"({sp_prev.label_}) {sp_prev} >({sp_prev.end}){sp} ({label})\")\n",
    "                elif sp.end < sp_prev.end:\n",
    "#                     sp_prev.start = sp.end\n",
    "                    label = sp_prev.label_\n",
    "                    sp_prev = doc[sp.end:sp_prev.end]\n",
    "                    sp_prev.label_ = label\n",
    "                    logging.debug(f\"({sp.label_}) {sp} >({sp.end}) {sp_prev} ({label})\")\n",
    "#                 remove_list.push_back(ii-1)\n",
    "#                 remove_list.append(ii-1)\n",
    "                else:\n",
    "                    raise ValueError(f\"this.end == prev.end: ({sp}){sp.end}== ({sp_prev}){sp_prev.end}\")\n",
    "            elif abs(sp.end - sp.start) != abs(sp_prev.end - sp_prev.start): # looser criterion\n",
    "#             elif abs(sp.end - sp_prev.end) > abs(sp.start - sp_prev.start):\n",
    "                if tail_overhang < head_overhang:\n",
    "                    logging.debug(f\"tail overhang ({tail_overhang}) is shorter then head overhang({head_overhang})\")\n",
    "                else:\n",
    "                    logging.debug(f\"tail overhang ({tail_overhang}) is equal to head overhang({head_overhang})\")\n",
    "                logging.debug(f\"fixing heads\")\n",
    "                if sp.start < sp_prev.start:\n",
    "#                     sp.end = sp_prev.start\n",
    "                    label = sp.label_\n",
    "                    sp = doc[sp.start:sp_prev.start]\n",
    "                    sp.label_ =  label\n",
    "                    logging.debug(f\"({sp.label_}) {sp}< {sp_prev} ({sp_prev.label_})\")\n",
    "                else:\n",
    "#                     sp_prev.end = sp.start\n",
    "                    label = sp_prev.label_\n",
    "                    sp_prev = doc[sp_prev.start:sp.start]\n",
    "                    sp_prev.label_ = label\n",
    "                    logging.debug(f\"({sp_prev.label_}) {sp_prev}< {sp} ({sp.label_})\")\n",
    "#                 remove_list.push_back(ii)\n",
    "#                 remove_list.append(ii)\n",
    "            else: \n",
    "#                 logging.debug(f\"ties:({sp}) {sp.label_}:{sp._.type} -- {sp_prev.label_}:{sp_prev._.type}\")\n",
    "                if len(priority_dict)>0:\n",
    "                    rank_this = priority_dict.get(label_fn(sp.label_), 1e20)\n",
    "                    rank_prev = priority_dict.get(label_fn(sp_prev.label_), 1e20)\n",
    "                    if rank_this <= rank_prev:\n",
    "#                         sp_prev[\"label\"] = None\n",
    "#                         remove_list.push_back(ii-1)\n",
    "                        logging.debug(f\"ties:({sp}) keep {sp.label_} ({rank_this}) \"\n",
    "                                      f\"-- remove {sp_prev.label_} ({rank_prev})\")\n",
    "#                         logging.debug(f\"ties:({sp}) keep {sp.label_}:{sp._.type} ({rank_this}) \"\n",
    "#                                       f\"-- remove {sp_prev.label_}:{sp_prev._.type} ({rank_prev})\")\n",
    "                        remove_list.append(ii-1)\n",
    "                    else:\n",
    "#                         sp[\"label\"] = None\n",
    "                        logging.debug(f\"ties:({sp}) keep {sp_prev.label_} ({rank_prev}) \"\n",
    "                                      f\"-- remove {sp.label_} ({rank_this})\")\n",
    "                        remove_list.append(ii)\n",
    "                else:\n",
    "                    logging.debug(f\"ties:({sp}) keep both: {sp.label_}, {sp_prev.label_}\")\n",
    "#         print(\"ii\", ii, new_spans)\n",
    "        if sp_prev is not None:\n",
    "            new_spans[-1] = sp_prev\n",
    "        new_spans.append(sp)\n",
    "        sp_prev = sp\n",
    "        prev_ranges.append((sp.start, sp.end))\n",
    "        \n",
    "#     print(\"remove_list\", remove_list)\n",
    "    for ii in reversed(remove_list):\n",
    "        new_spans.pop(ii)\n",
    "    return new_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_overlaps_disjoin_spacy((spans), \n",
    "# #                                                                  key = lambda x : (x[\"start\"], x[\"end\"])),\n",
    "#                                              priority_dict={\"Marital_or_partnership_status\":0,\n",
    "#                                                \"Social_isolation\":1,\n",
    "#                                                \"Housing\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_overlapping_groups(spans):\n",
    "    doc = spans[0].doc\n",
    "    sp_prev = None\n",
    "#     cdef vector[int] remove_list\n",
    "    groups = defaultdict(lambda : set())\n",
    "    gr = 1\n",
    "    new_spans = []\n",
    "    for ii, sp in enumerate(spans):\n",
    "\n",
    "        if sp_prev is None:\n",
    "            sp_prev = sp\n",
    "            new_spans.append(sp)\n",
    "            continue\n",
    "\n",
    "        ov = overlap.overlap(sp.start, sp.end, sp_prev.start, sp_prev.end)\n",
    "        if ov > 0.0:\n",
    "            groups[gr].add(sp_prev)\n",
    "            groups[gr].add(sp)\n",
    "        else:\n",
    "            gr+=1\n",
    "            groups[gr].add(sp)\n",
    "        sp_prev = sp\n",
    "    return groups\n",
    "\n",
    "def _split_group_with_matcher(spans_):\n",
    "    \"\"\"input: a group of overlapping labelled spans (1, 2, or more)\n",
    "    output: disjoint labelled spans separated using pattern matching\n",
    "    \"\"\"\n",
    "    if len(spans_)==1:\n",
    "        return spans_\n",
    "    start_ = min(spans_, key=lambda x: x.start).start\n",
    "    end_ = max(spans_, key=lambda x: x.end).end\n",
    "    labels = set([sp.label_ for sp in spans_])\n",
    "    sp_ = spans_.pop()\n",
    "#     if hasattr(sp_._, \"type\"):\n",
    "#         types = {}\n",
    "#         for sp in spans_:\n",
    "#             types[sp.label_] = sp._.type\n",
    "    snippet = sp_.doc[start_:end_]\n",
    "    spans_ = split_overlapping_spans(snippet, labels=labels)\n",
    "\n",
    "#     if hasattr(sp_._, \"type\"):\n",
    "#         for sp_ in spans_:\n",
    "#             sp_._.type = types[sp_.label_]\n",
    "    return spans_\n",
    "\n",
    "def split_overlaps_matcher_spacy(spans, priority_dict = {}):\n",
    "    groups = _collect_overlapping_groups(spans)\n",
    "        \n",
    "    result = []\n",
    "    for gr, spans_ in groups.items():\n",
    "        spans_ = _split_group_with_matcher(spans_)\n",
    "        result.extend(spans_)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii = 0\n",
    "# for entry in data:\n",
    "#     for ent in entry[\"entities\"]:\n",
    "#         if ent[\"label\"] == \"Housing\":\n",
    "#             print(ent[\"type\"], ent[\"text\"], sep=\" | \")\n",
    "#             if ent[\"type\"]==\"Marginally housed\":# and ent[\"text\"] == \"apartment\":\n",
    "#                 ii+=1\n",
    "#                 if ii>2:\n",
    "#                     raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_note(entry):\n",
    "    doc = nlp(entry[\"text\"])\n",
    "    doc._.note_id = entry[\"note_id\"]\n",
    "    doc._.id = str(entry[\"note_id\"]) + \"_\" + entry[\"annotator\"]\n",
    "    doc._.dataset = note_id_set.loc[entry[\"note_id\"], \"set\"]\n",
    "    doc._.fold = int(note_id_set.loc[entry[\"note_id\"], \"fold\"])\n",
    "    doc._.annotator = entry[\"annotator\"]\n",
    "    doc._.note_type = entry[\"note_type\"]\n",
    "    \n",
    "    if len(entry[\"entities\"])==0:\n",
    "        return None\n",
    "    entities = trim_whitespaces_dict(entry[\"entities\"], doc.text)\n",
    "    entities = sorted(entities, key = lambda x : (x[\"start\"], x[\"end\"]))\n",
    "    spans = [doc.char_span(ent[\"start\"], ent[\"end\"], label=ent[\"label\"] + \": \" + ent[\"type\"],\n",
    "                           vector=None, alignment_mode=\"expand\")\n",
    "                  for ent in entities\n",
    "            ]\n",
    "    new_spans = []\n",
    "    for sp, ent in zip(spans, entities):\n",
    "\n",
    "        if sp.label_ == \"Marital_or_partnership_status: Partner relationship problem\" and \\\n",
    "            sp.text in {\"husband\", \"wife\", \"partner\", \"spouse\", \"child\", \"son\", \"daughter\"}:\n",
    "            raise Exception\n",
    "\n",
    "        if any((x in sp.text.lower() for x in [\"home\", \"housed\", \"lives at home\", \"apartment\", \"residence\",\n",
    "                               \"condo\", \"house\", \"lives in home\", \"one bedroom apartment\",\"relocated\",\n",
    "                              \"housing\"])) and \\\n",
    "                sp.label_ == \"Housing: NA\":\n",
    "            sp.label_ = \"Housing: Housed\"\n",
    "        elif any((x in sp.text.lower() for x in ['boyfriend', 'girlfriend', 'gf', 'bf', \"rdp\",])) and \\\n",
    "                sp.label_ == \"Marital_or_partnership_status: Single\":\n",
    "#             sp.label_ = \"Social_isolation: Has social support\"\n",
    "            sp.label_ == \"Marital_or_partnership_status: Partner in a relationship\"\n",
    "#             if sp is not None:\n",
    "#                 sp._.type = ent[\"type\"]\n",
    "        elif sp.label_ == \"Anxiety: Level of anxiety\": # only Sara used this label (3 times)\n",
    "            sp.label_ = \"Anxiety: Anxiety\"\n",
    "        elif sp.label_ == \"Social_isolation: Support system case management\": # only 1 label in this class\n",
    "            sp.label_ = \"Social_isolation: NA\"\n",
    "        elif (sp.label_ == \"Social_isolation: Lives with\") and (\n",
    "                (\"with\" not in sp.text.lower()) and \n",
    "                (\"w/\" not in sp.text.lower()) and \n",
    "                (\"into\" not in sp.text.lower())\n",
    "                ):\n",
    "            logging.debug(f\"original lives_with:'{sp}'--{sp.label_}\")\n",
    "            sp = recover_lives_with(sp)\n",
    "            logging.debug(f\"recovered lives_with:'{sp}'--{sp.label_}\")\n",
    "#                 break\n",
    "        if (sp.label_ != \"Social_isolation: Lives with\") and (\"Lives with\" in sp.text.lower()):\n",
    "            raise ValueError\n",
    "#                 break\n",
    "        new_spans.append(sp)\n",
    "    new_spans = sorted(new_spans, key = lambda x : ( x.end))\n",
    "    new_spans = clean_overlaps_disjoin_spacy((new_spans), \n",
    "#                                                                  key = lambda x : (x[\"start\"], x[\"end\"])),\n",
    "                                         priority_dict={\"Marital_or_partnership_status\":0,\n",
    "                                           \"Social_isolation\":1,\n",
    "                                           \"Housing\":2})\n",
    "\n",
    "    for sp in new_spans:\n",
    "        if sp.text.lower()==\"italian\":\n",
    "            raise Exception\n",
    "        if \"lives at\" == sp.text.lower() and sp.label_ == \"Social_isolation: Lives with\":\n",
    "            raise Exception\n",
    "#             if sp.label_ == \"Depression: PHQ-9\":\n",
    "#                 raise Exception\n",
    "        if sp.label_ == \"Social_isolation: Housing: NA\":\n",
    "            raise Exception\n",
    "        if sp.label_ == \"Housing: NA\" and sp.text not in  {\"homes\", \"rent\", \"living situation\"}:\n",
    "            raise Exception\n",
    "        if sp.label_ == \"Marital_or_partnership_status: Partner relationship problem\" and \\\n",
    "            sp.text in {\"husband\", \"wife\", \"partner\", \"spouse\", \"child\", \"son\", \"daughter\"}:\n",
    "            raise Exception\n",
    "        if sp.label_ == \"Social_isolation: Lives alone\" and sp.text == \"Lives at\":\n",
    "            raise Exception\n",
    "\n",
    "        if any((x in sp.text.lower() for x in ['boyfriend', 'girlfriend', 'gf', 'bf'])) and \\\n",
    "                sp.label_ == \"Marital_or_partnership_status: Single\":\n",
    "            raise Exception\n",
    "#         new_spans = sorted(new_spans, key = lambda x : (x.start, x.end))\n",
    "    filtered_spans = spacy.util.filter_spans([sp for sp in new_spans if sp is not None]) \n",
    "    if len(filtered_spans) < len(new_spans):\n",
    "        removed_ents = list(set(new_spans) - set(filtered_spans))\n",
    "        for ent in removed_ents:\n",
    "            logging.warning(f\"{ii}\\tremoved:\\t{ent}\\t{ent.label_}\")\n",
    "        raise Exception\n",
    "\n",
    "    doc.set_ents((filtered_spans))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# span_utils.get_ent_df(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# span = doc[1907:1912]\n",
    "# print(span.text.strip(), \"\\n\", span.start_char+1, span.end_char+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# span = doc[1943:1946]\n",
    "# print(span.text.strip(), \"\\n\", span.start_char+1, \"~\", span.end_char+1, sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapping = {\"Marital_or_partnership_status: Common law partnership\": \"Marital_or_partnership_status: Partner\",\n",
    "\"Marital_or_partnership_status: Divorce\": \"Marital_or_partnership_status: Divorced\",\n",
    " \"Anxiety: Anxiety: mood/finding\": \"Anxiety: Signs and symptoms of anxiety\",\n",
    " \"Depression: Depressed mood\": \"Depression: Symptoms of depression\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "break_ = False\n",
    "for ii, entry in enumerate(data):\n",
    "    if ii < 0:\n",
    "        continue\n",
    "    if ii%50 == 0:\n",
    "        print(ii, end=\"\\t\")\n",
    "    if note_ids.loc[entry[\"note_id\"], \"n_entities_max\"] == 0:\n",
    "        continue\n",
    "    doc = nlp(entry[\"text\"])\n",
    "    doc._.note_id = entry[\"note_id\"]\n",
    "    doc._.id = str(entry[\"note_id\"]) + \"_\" + entry[\"annotator\"]\n",
    "    doc._.dataset = note_id_set.loc[entry[\"note_id\"], \"set\"]\n",
    "    doc._.fold = int(note_id_set.loc[entry[\"note_id\"], \"fold\"])\n",
    "    doc._.annotator = entry[\"annotator\"]\n",
    "    doc._.note_type = entry[\"note_type\"]\n",
    "    try:\n",
    "        if len(entry[\"entities\"])==0:\n",
    "            continue\n",
    "        entities = trim_whitespaces_dict(entry[\"entities\"], doc.text)\n",
    "        entities = sorted(entities, key = lambda x : (x[\"start\"], x[\"end\"]))\n",
    "        spans = [doc.char_span(ent[\"start\"], ent[\"end\"], label=ent[\"label\"] + \": \" + ent[\"type\"],\n",
    "                               vector=None, alignment_mode=\"expand\")\n",
    "                      for ent in entities\n",
    "                ]\n",
    "        new_spans = []\n",
    "        for sp, ent in zip(spans, entities):\n",
    "            \n",
    "            if sp.label_ == \"Marital_or_partnership_status: Partner relationship problem\" and \\\n",
    "                sp.text in {\"husband\", \"wife\", \"partner\", \"spouse\", \"child\", \"son\", \"daughter\"}:\n",
    "                raise Exception\n",
    "                \n",
    "            if any((x in sp.text.lower() for x in [\"home\", \"housed\", \"lives at home\", \"apartment\", \"residence\",\n",
    "                                   \"condo\", \"house\", \"lives in home\", \"one bedroom apartment\",\"relocated\",\n",
    "                                  \"housing\"])) and \\\n",
    "                    sp.label_ == \"Housing: NA\":\n",
    "                sp.label_ = \"Housing: Housed\"\n",
    "                \n",
    "            if (\"section 8\" in sp.text.lower()) or \\\n",
    "                (\"public housing\" in sp.text.lower()):\n",
    "                sp.label_ = \"Housing: Subsidized housing\"\n",
    "                \n",
    "            if (\"St Joseph's apartment\" in sp.text) or (\"group home\" in sp.text.lower()):\n",
    "                sp.label_ = \"Housing: lives in facility\"\n",
    "                \n",
    "            if any((x in sp.text.lower() for x in ['boyfriend', 'girlfriend', 'gf', 'bf', \"regdompart\", \"rdp\"])):# and \\\n",
    "                sp.label_ = \"Marital_or_partnership_status: Partner\"\n",
    "#             if sp.label_ == \"Marital_or_partnership_status: Common law partnership\":\n",
    "#                 sp.label_ = \"Marital_or_partnership_status: Partner\"\n",
    "\n",
    "            if sp.label_.startswith(\"Marital_or_partnership_status:\") and \"widow\" in sp.text.lower():\n",
    "                sp.label_ = \"Marital_or_partnership_status: Widowed\"\n",
    "\n",
    "            if sp.label_.startswith(\"Marital_or_partnership_status:\") and \"divorce\" in sp.text.lower():\n",
    "                sp.label_ = \"Marital_or_partnership_status: Divorced\"\n",
    "                \n",
    "            if sp.label_ == \"Depression: NA\" and sp.text.lower() == \"depression\":\n",
    "                sp.label_ == \"Depression: Symptoms of depression\"\n",
    "        \n",
    "            if sp.label_ == \"Anxiety: NA\" and sp.text.lower() == \"anxiety\":\n",
    "                sp.label_ == \"Anxiety: Signs and symptoms of anxiety\"\n",
    "                \n",
    "            if (sp.label_ == \"Social_isolation: Lives with\") and (\n",
    "                    (\"with\" not in sp.text.lower()) and \n",
    "                    (\"w/\" not in sp.text.lower()) and \n",
    "                    (\"into\" not in sp.text.lower())\n",
    "                    ):\n",
    "                logging.debug(f\"original lives_with:'{sp}'--{sp.label_}\")\n",
    "                sp = recover_lives_with(sp)\n",
    "                logging.debug(f\"recovered lives_with:'{sp}'--{sp.label_}\")\n",
    "#                 break\n",
    "            if (sp.label_ != \"Social_isolation: Lives with\") and (\"Lives with\" in sp.text.lower()):\n",
    "                raise ValueError\n",
    "#                 break\n",
    "            # ======= Remap labels\n",
    "            sp.label_ = remapping.get(sp.label_, sp.label_)\n",
    "        \n",
    "            new_spans.append(sp)\n",
    "        new_spans = sorted(new_spans, key = lambda x : ( x.end))\n",
    "        new_spans = clean_overlaps_disjoin_spacy((new_spans), \n",
    "#                                                                  key = lambda x : (x[\"start\"], x[\"end\"])),\n",
    "                                             priority_dict={\"Marital_or_partnership_status\":0,\n",
    "                                               \"Social_isolation\":1,\n",
    "                                               \"Housing\":2})\n",
    "#         doc._.entities = new_spans\n",
    "        for sp in new_spans:\n",
    "            if sp.text.lower()==\"italian\":\n",
    "                raise Exception\n",
    "            if \"lives at\" == sp.text.lower() and sp.label_ == \"Social_isolation: Lives with\":\n",
    "                raise Exception\n",
    "#             if sp.label_ == \"Depression: PHQ-9\":\n",
    "#                 raise Exception\n",
    "            if sp.label_ == \"Social_isolation: Housing: NA\":\n",
    "                raise Exception\n",
    "            if sp.label_ == \"Housing: NA\" and sp.text not in  {\"homes\", \"rent\", \"living situation\"}:\n",
    "                raise Exception\n",
    "            if sp.label_ == \"Marital_or_partnership_status: Partner relationship problem\" and \\\n",
    "                sp.text in {\"husband\", \"wife\", \"partner\", \"spouse\", \"child\", \"son\", \"daughter\"}:\n",
    "                raise Exception\n",
    "            if sp.label_ == \"Social_isolation: Lives alone\" and sp.text == \"Lives at\":\n",
    "                raise Exception\n",
    "                \n",
    "            if any((x in sp.text.lower() for x in ['boyfriend', 'girlfriend', 'gf', 'bf'])) and \\\n",
    "                    sp.label_ == \"Marital_or_partnership_status: Single\":\n",
    "                raise Exception\n",
    "#         new_spans = sorted(new_spans, key = lambda x : (x.start, x.end))\n",
    "        filtered_spans = spacy.util.filter_spans([sp for sp in new_spans \n",
    "                                                  if (sp is not None) and (sp.label_ not in {\"Misc: NA\", })]) \n",
    "        if len(filtered_spans) < len(new_spans):\n",
    "            removed_ents = list(set(new_spans) - set(filtered_spans))\n",
    "            for ent in removed_ents:\n",
    "                logging.warning(f\"{ii}\\tremoved:\\t{ent}\\t{ent.label_}\")\n",
    "#             raise Exception\n",
    "            \n",
    "        doc.set_ents((filtered_spans))\n",
    "#         break\n",
    "    except Exception as ee:\n",
    "        print(ii)\n",
    "        raise ee\n",
    "    docs.append(doc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok2vec = dict(nlp.pipeline)[\"tok2vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ = deepcopy(doc)\n",
    "# scores = tok2vec.predict(docs)\n",
    "\n",
    "if sp.label_.startswith(\"Marital_or_partnership_status:\") and \"widow\" in sp.text.lower():\n",
    "    sp.label_ = \"Marital_or_partnership_status: Widowed\"\n",
    "\n",
    "if sp.label_.startswith(\"Marital_or_partnership_status:\") and \"divorce\" in sp.text.lower():\n",
    "    sp.label_ = \"Marital_or_partnership_status: Divorced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_spacy = []\n",
    "break_ = False\n",
    "for doc in docs:\n",
    "    dict_ = {kk:vv for kk,vv in doc.to_json().items() if kk in ['text', 'ents']}\n",
    "    for sp, ent in zip(doc.ents, dict_[\"ents\"]):\n",
    "        ent[\"type\"] = sp._.type\n",
    "        ent[\"text\"] = sp.text\n",
    "        if sp._.type==\"Has social support\" and sp.label_ != \"Social_isolation\":\n",
    "            print(\"!!!\")\n",
    "            break_ = True\n",
    "            break\n",
    "    dict_.update({kk:getattr(doc._, kk) for kk in dir(doc._) if isinstance(getattr(doc._, kk), (str, int))})\n",
    "    data_after_spacy.append(dict_)\n",
    "    if break_:\n",
    "        break\n",
    "len(data_after_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabel_dict = {\"Insurance_status: Able to access healthcare services\": \"Insurance_status: NA\",\n",
    "                \"Insurance_status: Education about insurance coverage\": \"Insurance_status: NA\",\n",
    "                \"Housing: Housed\":\"Housing: Stably housed\",\n",
    "                # \"Anxiety: Anxiety: mood/finding\" : \"Anxiety: Anxiety\",\n",
    "                \"Marital_or_partnership_status: Single\": \"Marital_or_partnership_status: Single person\",\n",
    "                \"Social_isolation: Support system case management\" : \"Social_isolation: NA\",\n",
    "                \"Food: Food Insecurity\": \"Food: Provision of food\",\n",
    "                \"Financial_strain: Low Income\": \"Financial_strain: Financial problem\",\n",
    "                \"Financial_strain: Unable to afford visit copayment\": \"Financial_strain: Financial problem\",\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in relabel_dict:\n",
    "            ent.label_ = relabel_dict[ent.label_]\n",
    "        elif \"Depression: Mixed anxiety and depressive disorder\" ==  ent.label_:\n",
    "            break\n",
    "        doc_ents.append(ent)\n",
    "        doc.set_ents(doc_ents)\n",
    "#             print(ent.label_)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from span_utils import get_ent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tok_to_dict(ent, snippet=None):\n",
    "    result = {\"text\": ent.text,\n",
    "              \"start\": ent.start_char, \"end\": ent.end_char,\n",
    "              \"start_tok\": ent.start, \"end_tok\": ent.end,\n",
    "              \"label\": ent.label_}\n",
    "\n",
    "    for ext in dir(ent._):\n",
    "        if ext not in {\"has\", \"set\", \"get\"}:\n",
    "            val = getattr(ent._, ext)\n",
    "            if val is not None:\n",
    "                result[ext] = val\n",
    "\n",
    "    if hasattr(ent._, \"type\"):\n",
    "        result[\"type\"] = ent._.type\n",
    "\n",
    "    if snippet:\n",
    "        doc = ent.doc\n",
    "        result[\"snippet\"] = doc[max(0, ent.start-snippet):min(ent.end+snippet, len(doc))].text\n",
    "    return result\n",
    "\n",
    "def get_ent_dict(ents, snippet = None):\n",
    "    return [_tok_to_dict(ent, snippet=snippet) for ent in ents]\n",
    "\n",
    "\n",
    "def get_ent_df(ents, snippet = None):\n",
    "    ents = get_ent_dict(ents, snippet = snippet)\n",
    "    return pd.DataFrame(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ents = []\n",
    "for doc in docs:\n",
    "    df_ = get_ent_df(doc.ents, snippet=10)\n",
    "    df_[\"id\"] = doc._.id\n",
    "    df_ents.append(df_)\n",
    "\n",
    "df_ents = pd.concat(df_ents)\n",
    "df_ents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open results/label_counts_lvl2_normalized.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ents[\"label\"].value_counts().sort_index().to_frame().to_csv(\"results/label_counts_lvl2_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ents[\"label\"].value_counts()[df_ents[\"label\"].value_counts()<30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ents[df_ents[\"label\"] == \"Housing: lives in facility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ents[df_ents[\"label\"] == \"Anxiety: NA\"][\"text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ents[(df_ents[\"label\"] == \"Anxiety: NA\") &\n",
    "       (df_ents[\"text\"].str.lower() == \"anxiety\")][\"snippet\"].str.strip().str.replace(\"\\n\",\" \").map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([doc._.note_id for doc in docs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docbins = defaultdict(lambda: DocBin(store_user_data=True))\n",
    "for doc in docs:\n",
    "    docbins[doc._.dataset].add(doc)\n",
    "\n",
    "for set_, docbin_ in docbins.items():\n",
    "    docbin_.to_disk(f\"./data/spacy-level-2/sdoh-cocoa-skippy-no-empty-2022-01-11-{set_}.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(6):\n",
    "    docbins = defaultdict(lambda: DocBin(store_user_data=True))\n",
    "    for doc in docs:\n",
    "        if doc._.fold == fold:\n",
    "#             print(fold, \"val\")\n",
    "            docbins[\"val\"].add(doc)\n",
    "        elif (doc._.fold  + 1 ) % 6 == fold:\n",
    "#             print(fold, \"test\")\n",
    "            docbins[\"test\"].add(doc)\n",
    "        else:\n",
    "            docbins[\"train\"].add(doc)\n",
    "            \n",
    "    for set_, docbin_ in docbins.items():\n",
    "        docbin_.to_disk(f\"./data/spacy-level-2/sdoh-cocoa-skippy-no-empty-2022-01-11-fold-{fold}-{set_}.spacy\")\n",
    "#     docbins = defaultdict(lambda: DocBin(store_user_data=True))\n",
    "#     for doc in docs:\n",
    "#         docbins[doc._.dataset].add(doc)\n",
    "\n",
    "#     for set_, docbin_ in docbins.items():\n",
    "#         docbin_.to_disk(f\"./data/sdoh-cocoa-skippy-no-empty-{set_}.spacy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./data/spacy-level-2/sdoh-cocoa-skippy-no-empty-2022-01-11-fold-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sdoh-cocoa-skippy-no-empty-2022-01-11.jsonl\", \"w\") as fh:\n",
    "    for doc in docs:\n",
    "        dict_ = {kk:vv for kk,vv in doc.to_json().items() if kk in ['text', 'ents']}\n",
    "        for sp, ent in zip(doc.ents, dict_[\"ents\"]):\n",
    "            ent[\"type\"] = sp._.type\n",
    "            ent[\"text\"] = sp.text\n",
    "        dict_.update({kk:getattr(doc._, kk) for kk in dir(doc._) if isinstance(getattr(doc._, kk), (str, int))})\n",
    "        fh.write(json.dumps(dict_) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {kk:getattr(doc._, kk) for kk in dir(doc._) if isinstance(getattr(doc._, kk), (str, int))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc._.fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head -n1 \"./data/sdoh-cocoa-skippy-no-empty.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for doc in docs:\n",
    "    entities.extend([{\"label\":ent.label_, \"text\":str(ent), \"doc_id\": doc._.id} for ent in doc.ents])\n",
    "df_entities = pd.DataFrame(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_entities.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities.groupby(\"label\")[\"text\"].value_counts()[:20]#[\"pain_and_disability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.hist([len(doc.ents) for doc in docs], bins=np.arange(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotator\n",
    "# Emilia     688\n",
    "# Matt      3032\n",
    "# Rafael    1173\n",
    "# Sam        166\n",
    "# Sara      2963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:49.936161Z",
     "start_time": "2021-04-01T17:44:49.931701Z"
    }
   },
   "outputs": [],
   "source": [
    "annotations_raw = pd.DataFrame(tag_list)\n",
    "annotations_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_raw.note_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations_raw.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(annotations_raw.label, annotations_raw.note_type,)\n",
    "# #.divide(annotations_raw.note_type.value_counts(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse `note_id` and expert name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:50.158072Z",
     "start_time": "2021-04-01T17:44:49.980493Z"
    }
   },
   "outputs": [],
   "source": [
    "annotations = pd.concat([annotations_raw, \n",
    "           annotations_raw.filename.apply(lambda fn: pd.Series(parse_filename(fn)))],1)\n",
    "annotations[\"annotator\"] = annotations.annotator.str.title().map(lambda x: {\"Raf\": \"Rafael\",\n",
    "                                                                           \"Mat\": \"Matt\"}.get(x,x))\n",
    "annotations = annotations.sort_values([\"filename\", \"start\"])\n",
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for note_id, vv in annotations.groupby(\"note_id\"):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.groupby(\"start\")[\"end\"].agg(lambda x:x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.sort_values(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vv.sort_values(\"start\")[\"span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "# reduce( lambda x,y: x.overlaps(y), vv.sort_values(\"start\")[\"span\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:50.161639Z",
     "start_time": "2021-04-01T17:44:50.159652Z"
    }
   },
   "outputs": [],
   "source": [
    "today_iso = date.today().isoformat()\n",
    "outdir = f\"results/pt-sample-2020-02-12-analysis-{today_iso}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:50.166589Z",
     "start_time": "2021-04-01T17:44:50.163961Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:50.176069Z",
     "start_time": "2021-04-01T17:44:50.168764Z"
    }
   },
   "outputs": [],
   "source": [
    "out_fn = f\"{outdir}/annotations.csv\"\n",
    "print(out_fn)\n",
    "annotations.to_csv(out_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:44:50.179362Z",
     "start_time": "2021-04-01T17:44:50.177533Z"
    }
   },
   "outputs": [],
   "source": [
    "# annotations[annotations.note_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3.9@python",
   "language": "python",
   "name": "nlp3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
